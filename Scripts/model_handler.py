from vllm import LLM, SamplingParams
from typing import List, Dict, Optional
from config import Config
from utils import format_llama3_prompt  
import logging

logger = logging.getLogger(__name__)


class ModelHandler:
    """
    Wrapper class cho vLLM model.

    X·ª≠ l√Ω:
    - Load v√† kh·ªüi t·∫°o model
    - Single generation v·ªõi l·ªãch s·ª≠ h·ªôi tho·∫°i
    - Batch generation
    - Error handling v√† logging
    """

    def __init__(self):
        """Kh·ªüi t·∫°o vLLM model v·ªõi config t·ª´ Config class."""
        logger.info("ƒêang load model v·ªõi vLLM...")
        logger.info(f"‚úì ƒê∆∞·ªùng d·∫´n Model: {Config.MODEL_PATH}")
        logger.info(f"‚úì Dtype: {Config.DTYPE}")
        logger.info(f"‚úì GPU Memory Utilization: {Config.GPU_MEMORY_UTILIZATION}")
        logger.info(f"‚úì Max Model Length: {Config.MAX_MODEL_LEN}")
        logger.info(f"‚úì Max Num Seqs: {Config.MAX_NUM_SEQS}")
        logger.info(f"‚úì Enforce Eager: {Config.ENFORCE_EAGER}")

        try:
            self.llm = LLM(
                model=Config.MODEL_PATH,
                tensor_parallel_size=Config.TENSOR_PARALLEL_SIZE,
                gpu_memory_utilization=Config.GPU_MEMORY_UTILIZATION,
                max_model_len=Config.MAX_MODEL_LEN,
                trust_remote_code=True,
                dtype=Config.DTYPE,
                enforce_eager=Config.ENFORCE_EAGER,
                max_num_seqs=Config.MAX_NUM_SEQS,
                tokenizer_mode="auto",
            )

            logger.info("=" * 60)
            logger.info("‚úÖ Model ƒë√£ ƒë∆∞·ª£c load th√†nh c√¥ng!")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ load model: {e}")
            logger.error("Stack trace:", exc_info=True)
            raise RuntimeError(f"Load model th·∫•t b·∫°i: {e}")

    def _build_simple_prompt(self, system_prompt: str, user_input: str) -> str:
        """
        Build simple prompt cho lightweight generation.
        Kh√¥ng d√πng format_llama3_prompt ƒë·ªÉ nhanh h∆°n.
        """
        return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>

{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""

    def generate(
            self,
            system_prompt: str,
            user_input: str,
            conversation_history: Optional[List[Dict]] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
    ) -> str:
        """
        Generate response t·ª´ model v·ªõi h·ªó tr·ª£ l·ªãch s·ª≠ h·ªôi tho·∫°i.

        Args:
            system_prompt: H∆∞·ªõng d·∫´n system cho agent
            user_input: Message hi·ªán t·∫°i t·ª´ user
            conversation_history: List c√°c messages tr∆∞·ªõc ƒë√≥ (role, content)
            temperature: Temperature cho sampling (None = d√πng default)
            max_tokens: S·ªë tokens t·ªëi ƒëa ƒë·ªÉ generate (None = d√πng default)

        Returns:
            Text response ƒë√£ ƒë∆∞·ª£c generate

        Raises:
            Exception: N·∫øu generation th·∫•t b·∫°i
        """

        # S·ª≠ d·ª•ng defaults n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if temperature is None:
            temperature = Config.DEFAULT_TEMPERATURE
        if max_tokens is None:
            max_tokens = Config.DEFAULT_MAX_TOKENS

        try:
            # ‚úÖ SMART HANDLING: Detect short inputs v√† d√πng lightweight prompt
            user_input_clean = user_input.strip().lower()
            
            # Ph√¢n lo·∫°i intent c·ªßa short inputs
            greeting_words = ['hi', 'hey', 'hello', 'ch√†o', 'chao', 'xin ch√†o', 'alo', 'a lo', 'h√™ l√¥']
            farewell_words = ['bye', 'goodbye', 't·∫°m bi·ªát', 'tam biet', 'h·∫πn g·∫∑p l·∫°i', 'see you', 'bai bai']
            acknowledgment_words = ['ok', 'oke', 'okay', 'uh', 'um', 'uhm', '√†', '·ªù', '·ª´']
            thanks_words = ['thanks', 'thank you', 'c·∫£m ∆°n', 'cam on', 'c√°m ∆°n', 'thank', 'c·∫£m ∆°n em']
            
            is_greeting = user_input_clean in greeting_words
            is_farewell = user_input_clean in farewell_words
            is_thanks = user_input_clean in thanks_words
            is_acknowledgment = user_input_clean in acknowledgment_words
            
            # ‚úÖ N·∫øu l√† short social input ‚Üí D√πng lightweight prompt
            if is_greeting or is_farewell or is_thanks:
                logger.info(f"üé≠ Detected social input: '{user_input}' (type: {is_greeting and 'greeting' or is_farewell and 'farewell' or 'thanks'})")
                
                # X√°c ƒë·ªãnh lo·∫°i
                if is_greeting:
                    intent_type = "greeting (ch√†o h·ªèi)"
                elif is_farewell:
                    intent_type = "farewell (t·∫°m bi·ªát)"
                else:
                    intent_type = "thanks (c·∫£m ∆°n)"
                
                # Lightweight prompt - ƒë·ªÉ model t·ª± s√°ng t·∫°o
                lightweight_system = f"""B·∫°n l√† tr·ª£ l√Ω th√¢n thi·ªán c·ªßa S∆°n ƒê·ª©c D∆∞∆°ng.

Kh√°ch v·ª´a {intent_type}. H√£y tr·∫£ l·ªùi T·ª∞ NHI√äN, NG·∫ÆN G·ªåN (1-2 c√¢u).

G·ª£i √Ω:
- N·∫øu ch√†o h·ªèi: Ch√†o l·∫°i th√¢n thi·ªán, h·ªèi c·∫ßn gi√∫p g√¨
- N·∫øu t·∫°m bi·ªát: Ch√∫c t·ªët l√†nh, m·ªùi quay l·∫°i
- N·∫øu c·∫£m ∆°n: ƒê√°p l·∫°i l·ªãch s·ª±

X∆∞ng "em" (b·∫°n), "anh/ch·ªã" (kh√°ch). T·ª∞ NHI√äN, ƒë·ª´ng gi·ªëng nhau m·ªói l·∫ßn."""

                # Generate v·ªõi lightweight prompt
                response = self.llm.generate(
                    [self._build_simple_prompt(lightweight_system, user_input)],
                    SamplingParams(
                        temperature=0.8,  # Cao h∆°n ƒë·ªÉ ƒëa d·∫°ng
                        top_p=0.95,
                        max_tokens=100,
                        stop=["<|eot_id|>", "<|end_of_text|>", "\n\n"]
                    )
                )
                
                text = response[0].outputs[0].text.strip()
                
                if text:
                    logger.info(f"‚úÖ Generated social response: {len(text)} chars")
                    return text
                
                # Fallback n·∫øu generate r·ªóng
                logger.warning("‚ö†Ô∏è Lightweight generation failed, using fallback")
            
            # ‚úÖ Acknowledgment (ok, oke): Ph·ª• thu·ªôc context
            if is_acknowledgment:
                # Ki·ªÉm tra c√≥ conversation history kh√¥ng
                if not conversation_history or len(conversation_history) == 0:
                    logger.info(f"‚ùì Acknowledgment without context: '{user_input}'")
                    
                    # Lightweight prompt cho acknowledgment
                    ack_prompt = """B·∫°n l√† tr·ª£ l√Ω c·ªßa S∆°n ƒê·ª©c D∆∞∆°ng.

Kh√°ch ch·ªâ n√≥i "ok/oke" m√† kh√¥ng c√≥ ng·ªØ c·∫£nh tr∆∞·ªõc ƒë√≥.

H√£y h·ªèi l·∫°i xem kh√°ch c·∫ßn gi√∫p g√¨. T·ª± nhi√™n, ng·∫Øn g·ªçn 1 c√¢u."""
                    
                    response = self.llm.generate(
                        [self._build_simple_prompt(ack_prompt, user_input)],
                        SamplingParams(
                            temperature=0.7,
                            max_tokens=80,
                            stop=["<|eot_id|>", "<|end_of_text|>"]
                        )
                    )
                    
                    text = response[0].outputs[0].text.strip()
                    if text:
                        return text
                else:
                    # C√≥ context ‚Üí ƒê·ªÉ x·ª≠ l√Ω b√¨nh th∆∞·ªùng
                    logger.info(f"‚úÖ Acknowledgment with context, continue normally")
            
            # ‚úÖ Input qu√° ng·∫Øn v√† kh√¥ng c√≥ √Ω nghƒ©a
            if len(user_input_clean) <= 2 and user_input_clean not in ['hi', '∆°i', '√†', '√™']:
                logger.warning(f"‚ö†Ô∏è Input qu√° ng·∫Øn: '{user_input}'")
                
                unclear_prompt = """B·∫°n l√† tr·ª£ l√Ω c·ªßa S∆°n ƒê·ª©c D∆∞∆°ng.

Kh√°ch g·ª≠i tin nh·∫Øn qu√° ng·∫Øn/kh√¥ng r√µ r√†ng.

H√£y l·ªãch s·ª± h·ªèi l·∫°i. Ng·∫Øn g·ªçn, t·ª± nhi√™n."""
                
                response = self.llm.generate(
                    [self._build_simple_prompt(unclear_prompt, user_input)],
                    SamplingParams(
                        temperature=0.7,
                        max_tokens=60,
                        stop=["<|eot_id|>", "<|end_of_text|>"]
                    )
                )
                
                text = response[0].outputs[0].text.strip()
                if text:
                    return text
            
            # ‚úÖ Ti·∫øp t·ª•c x·ª≠ l√Ω b√¨nh th∆∞·ªùng cho c√°c input kh√°c
            # Chu·∫©n b·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i (ch·ªâ gi·ªØ c√°c messages g·∫ßn ƒë√¢y)
            history = []
            if conversation_history:
                # Gi·ªØ 10 messages cu·ªëi ƒë·ªÉ qu·∫£n l√Ω ƒë·ªô d√†i context
                history = conversation_history[-10:]

            # Th√™m user message hi·ªán t·∫°i
            history.append({"role": "user", "content": user_input})

            # ‚úÖ FIXED: Format prompt v·ªõi Llama 3 template ƒê√öNG CHU·∫®N
            prompt = format_llama3_prompt(system_prompt, history)

            # Log ƒë·ªô d√†i prompt
            logger.debug(f"üìù ƒê·ªô d√†i prompt: {len(prompt)} k√Ω t·ª±, {len(history)} turns")
            logger.debug(f"üìù Prompt preview (first 300 chars): {prompt[:300]}...")
            
            # Debug: Log to√†n b·ªô prompt n·∫øu c·∫ßn
            if logger.level <= logging.DEBUG:
                logger.debug("=" * 60)
                logger.debug("FULL PROMPT:")
                logger.debug("=" * 60)
                logger.debug(prompt)
                logger.debug("=" * 60)

            # ‚úÖ FIXED: Stop tokens cho Llama 3
            llama3_stop_tokens = [
                "<|eot_id|>",           # End of turn
                "<|end_of_text|>",      # End of text
                "<|start_header_id|>",  # Kh√¥ng ƒë·ªÉ model t·ª± t·∫°o header m·ªõi
            ]

            # Tham s·ªë sampling
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=Config.TOP_P,
                max_tokens=max_tokens,
                repetition_penalty=Config.REPETITION_PENALTY,
                stop=llama3_stop_tokens,  # ‚úÖ FIXED: D√πng stop tokens ƒë√∫ng cho Llama 3
            )

            logger.debug(f"‚öôÔ∏è Sampling params: temp={temperature}, max_tokens={max_tokens}, top_p={Config.TOP_P}")
            logger.debug(f"‚öôÔ∏è Stop tokens: {llama3_stop_tokens}")

            # Generate
            logger.debug("üîÑ ƒêang generate...")
            outputs = self.llm.generate([prompt], sampling_params)

            # DEBUG: Log chi ti·∫øt output
            logger.info(f"‚úÖ Generated output count: {len(outputs)}")
            
            if outputs and len(outputs) > 0:
                raw_text = outputs[0].outputs[0].text
                finish_reason = outputs[0].outputs[0].finish_reason
                
                logger.info(f"üìä Raw output length: {len(raw_text)} chars")
                logger.info(f"üìä Finish reason: {finish_reason}")
                logger.info(f"üìä Raw output preview (first 500 chars): '{raw_text[:500]}'")
                
                # ‚úÖ FIXED: L√†m s·∫°ch output
                # Lo·∫°i b·ªè special tokens n·∫øu c√≥
                cleaned_text = raw_text
                for token in llama3_stop_tokens:
                    cleaned_text = cleaned_text.replace(token, '')
                
                cleaned_text = cleaned_text.strip()
                
                # Ki·ªÉm tra n·∫øu output r·ªóng HO·∫∂C ch·ªâ ch·ª©a meta-instructions
                if not cleaned_text or len(cleaned_text) == 0:
                    logger.warning("‚ö†Ô∏è Model generate response R·ªñNG sau khi clean!")
                    return "Xin l·ªói, em ch∆∞a th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ph√π h·ª£p. Anh/ch·ªã th·ª≠ h·ªèi l·∫°i ƒë∆∞·ª£c kh√¥ng ·∫°?"
                
                # ‚úÖ FIXED: Ph√°t hi·ªán meta-instructions
                meta_keywords = [
                    "n·∫øu nh·∫≠n tag",
                    "ta s·∫Ω tr·∫£ l·ªùi",
                    "ta c√≥ th·ªÉ d·ª±a v√†o",
                    "ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi",
                    "quy t·∫Øc:",
                    "quy_t·∫Øc",
                    "nhi·ªám v·ª•:",
                    "nhi·ªám_v·ª•",
                    "b∆∞·ªõc 1:",
                    "ph∆∞∆°ng ph√°p",
                    "v√≠ d·ª• tr·∫£ l·ªùi",
                    "l∆∞u √Ω:",
                    "trong ƒëo·∫°n ƒë·ªëi tho·∫°i",
                    "tr√™n ƒë√¢y l√† v√≠ d·ª•",
                    "#1.", "#2.", "#3.", "#4.",  # Headers
                    "quan_tr·ªçng:",
                ]
                
                cleaned_lower = cleaned_text.lower()
                has_meta = any(kw in cleaned_lower for kw in meta_keywords)
                
                if has_meta:
                    logger.warning("‚ö†Ô∏è Model ƒëang generate META-INSTRUCTIONS thay v√¨ tr·∫£ l·ªùi!")
                    logger.warning(f"‚ö†Ô∏è Detected keywords in output: {[kw for kw in meta_keywords if kw in cleaned_lower]}")
                    logger.warning(f"‚ö†Ô∏è Output preview: {cleaned_text[:300]}")
                    
                    # Th·ª≠ extract c√¢u tr·∫£ l·ªùi th·ª±c s·ª± (n·∫øu c√≥)
                    # T√¨m d√≤ng ƒë·∫ßu ti√™n kh√¥ng ph·∫£i l√† instruction
                    lines = cleaned_text.split('\n')
                    for line in lines:
                        line_clean = line.strip()
                        line_lower = line_clean.lower()
                        
                        # B·ªè qua d√≤ng l√† instruction
                        if any(kw in line_lower for kw in meta_keywords):
                            continue
                        
                        # B·ªè qua d√≤ng r·ªóng ho·∫∑c ch·ªâ c√≥ d·∫•u
                        if not line_clean or line_clean in ['-', '*', '‚Ä¢']:
                            continue
                        
                        # N·∫øu t√¨m ƒë∆∞·ª£c d√≤ng h·ª£p l·ªá, tr·∫£ v·ªÅ
                        if len(line_clean) > 10:
                            logger.info(f"‚úÖ Extracted valid response from meta output: {line_clean[:100]}")
                            return line_clean
                    
                    # Kh√¥ng t√¨m ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi h·ª£p l·ªá
                    return "Ch√†o anh/ch·ªã! Em l√† tr·ª£ l√Ω c·ªßa S∆°n ƒê·ª©c D∆∞∆°ng. Em c√≥ th·ªÉ gi√∫p g√¨ cho anh/ch·ªã ·∫°?"
                
                logger.info(f"‚úÖ Generated successfully: {len(cleaned_text)} chars")
                logger.debug(f"‚úÖ Final output: {cleaned_text[:200]}...")
                
                return cleaned_text
                
            else:
                logger.error("Model tr·∫£ v·ªÅ output array r·ªóng")
                return "Xin l·ªói, em g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu."

        except Exception as e:
            logger.error(f"L·ªói khi generate: {e}", exc_info=True)
            logger.error(f"User input was: {user_input}")
            logger.error(f"System prompt length: {len(system_prompt)}")
            return "Xin l·ªói, em g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa anh/ch·ªã."

    def batch_generate(
            self,
            prompts: List[str],
            system_prompt: str,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
    ) -> List[str]:
        """
        Batch generation cho nhi·ªÅu prompts c√πng l√∫c.

        L∆∞u √Ω: Kh√¥ng s·ª≠ d·ª•ng l·ªãch s·ª≠ h·ªôi tho·∫°i - t·ªët nh·∫•t cho c√°c queries ƒë∆°n gi·∫£n.

        Args:
            prompts: List c√°c chu·ªói user input
            system_prompt: H∆∞·ªõng d·∫´n system
            temperature: Temperature cho sampling
            max_tokens: S·ªë tokens t·ªëi ƒëa m·ªói generation

        Returns:
            List c√°c responses ƒë√£ generate (c√πng ƒë·ªô d√†i v·ªõi prompts)
        """

        # S·ª≠ d·ª•ng defaults n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if temperature is None:
            temperature = Config.DEFAULT_TEMPERATURE
        if max_tokens is None:
            max_tokens = Config.DEFAULT_MAX_TOKENS

        try:
            logger.info(f"üì¶ ƒêang batch generate {len(prompts)} prompts")

            # Format t·∫•t c·∫£ prompts v·ªõi Llama 3 format
            formatted = []
            for p in prompts:
                formatted.append(
                    format_llama3_prompt(  # ‚úÖ FIXED: D√πng format ƒë√∫ng
                        system_prompt,
                        [{"role": "user", "content": p}]
                    )
                )

            # Stop tokens cho Llama 3
            llama3_stop_tokens = [
                "<|eot_id|>",
                "<|end_of_text|>",
                "<|start_header_id|>",
            ]

            # Tham s·ªë sampling
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=Config.TOP_P,
                max_tokens=max_tokens,
                repetition_penalty=Config.REPETITION_PENALTY,
                stop=llama3_stop_tokens,  # ‚úÖ FIXED
            )

            # Generate batch
            outputs = self.llm.generate(formatted, sampling_params)

            # Tr√≠ch xu·∫•t v√† l√†m s·∫°ch t·∫•t c·∫£ texts
            results = []
            for o in outputs:
                text = o.outputs[0].text
                # Lo·∫°i b·ªè special tokens
                for token in llama3_stop_tokens:
                    text = text.replace(token, '')
                results.append(text.strip())

            logger.info(f"‚úÖ Batch generation ho√†n t·∫•t: {len(results)} responses")

            return results

        except Exception as e:
            logger.error(f"‚ùå L·ªói batch generation: {e}", exc_info=True)
            # Tr·∫£ v·ªÅ error messages cho t·∫•t c·∫£ prompts
            return ["L·ªói x·ª≠ l√Ω"] * len(prompts)

    def get_model_info(self) -> Dict:
        """
        L·∫•y th√¥ng tin v·ªÅ model ƒëang ƒë∆∞·ª£c load.
        
        Returns:
            Dict ch·ª©a th√¥ng tin model
        """
        return {
            "model_path": Config.MODEL_PATH,
            "dtype": Config.DTYPE,
            "max_model_len": Config.MAX_MODEL_LEN,
            "gpu_memory_utilization": Config.GPU_MEMORY_UTILIZATION,
            "tensor_parallel_size": Config.TENSOR_PARALLEL_SIZE,
            "max_num_seqs": Config.MAX_NUM_SEQS,
        }