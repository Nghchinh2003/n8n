from vllm import LLM, SamplingParams
from typing import List, Dict, Optional
from config import Config
from utils import format_llama3_prompt  # ‚úÖ FIXED: D√πng h√†m ch√≠nh th·ª©c
import logging

logger = logging.getLogger(__name__)


class ModelHandler:
    """
    Wrapper class cho vLLM model.

    X·ª≠ l√Ω:
    - Load v√† kh·ªüi t·∫°o model
    - Single generation v·ªõi l·ªãch s·ª≠ h·ªôi tho·∫°i
    - Batch generation
    - Error handling v√† logging
    """

    def __init__(self):
        """Kh·ªüi t·∫°o vLLM model v·ªõi config t·ª´ Config class."""
        logger.info("ƒêang load model v·ªõi vLLM...")
        logger.info(f"‚úì ƒê∆∞·ªùng d·∫´n Model: {Config.MODEL_PATH}")
        logger.info(f"‚úì Dtype: {Config.DTYPE}")
        logger.info(f"‚úì GPU Memory Utilization: {Config.GPU_MEMORY_UTILIZATION}")
        logger.info(f"‚úì Max Model Length: {Config.MAX_MODEL_LEN}")
        logger.info(f"‚úì Max Num Seqs: {Config.MAX_NUM_SEQS}")
        logger.info(f"‚úì Enforce Eager: {Config.ENFORCE_EAGER}")

        try:
            self.llm = LLM(
                model=Config.MODEL_PATH,
                tensor_parallel_size=Config.TENSOR_PARALLEL_SIZE,
                gpu_memory_utilization=Config.GPU_MEMORY_UTILIZATION,
                max_model_len=Config.MAX_MODEL_LEN,
                trust_remote_code=True,
                dtype=Config.DTYPE,
                enforce_eager=Config.ENFORCE_EAGER,
                max_num_seqs=Config.MAX_NUM_SEQS,
                tokenizer_mode="auto",
            )

            logger.info("=" * 60)
            logger.info("‚úÖ Model ƒë√£ ƒë∆∞·ª£c load th√†nh c√¥ng!")
            logger.info("=" * 60)

        except Exception as e:
            logger.error(f"‚ùå Kh√¥ng th·ªÉ load model: {e}")
            logger.error("Stack trace:", exc_info=True)
            raise RuntimeError(f"Load model th·∫•t b·∫°i: {e}")

    def generate(
            self,
            system_prompt: str,
            user_input: str,
            conversation_history: Optional[List[Dict]] = None,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
    ) -> str:
        """
        Generate response t·ª´ model v·ªõi h·ªó tr·ª£ l·ªãch s·ª≠ h·ªôi tho·∫°i.

        Args:
            system_prompt: H∆∞·ªõng d·∫´n system cho agent
            user_input: Message hi·ªán t·∫°i t·ª´ user
            conversation_history: List c√°c messages tr∆∞·ªõc ƒë√≥ (role, content)
            temperature: Temperature cho sampling (None = d√πng default)
            max_tokens: S·ªë tokens t·ªëi ƒëa ƒë·ªÉ generate (None = d√πng default)

        Returns:
            Text response ƒë√£ ƒë∆∞·ª£c generate

        Raises:
            Exception: N·∫øu generation th·∫•t b·∫°i
        """

        # S·ª≠ d·ª•ng defaults n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if temperature is None:
            temperature = Config.DEFAULT_TEMPERATURE
        if max_tokens is None:
            max_tokens = Config.DEFAULT_MAX_TOKENS

        try:
            # ‚úÖ ADDED: X·ª≠ l√Ω low-quality inputs
            user_input_clean = user_input.strip().lower()
            
            # N·∫øu input qu√° ng·∫Øn ho·∫∑c kh√¥ng c√≥ √Ω nghƒ©a, tr·∫£ v·ªÅ default response
            if len(user_input_clean) <= 3 or user_input_clean in ['ok', 'oke', 'ok...', '...', 'hi', 'hey']:
                logger.warning(f"‚ö†Ô∏è Input qu√° ng·∫Øn ho·∫∑c m∆° h·ªì: '{user_input}'")
                return "Ch√†o anh/ch·ªã! Em l√† tr·ª£ l√Ω c·ªßa S∆°n ƒê·ª©c D∆∞∆°ng. Em c√≥ th·ªÉ t∆∞ v·∫•n v·ªÅ s∆°n, keo d√°n v√† v·∫≠t li·ªáu x√¢y d·ª±ng. Anh/ch·ªã c·∫ßn em gi√∫p g√¨ ·∫°?"
            
            # Chu·∫©n b·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i (ch·ªâ gi·ªØ c√°c messages g·∫ßn ƒë√¢y)
            history = []
            if conversation_history:
                # Gi·ªØ 10 messages cu·ªëi ƒë·ªÉ qu·∫£n l√Ω ƒë·ªô d√†i context
                history = conversation_history[-10:]

            # Th√™m user message hi·ªán t·∫°i
            history.append({"role": "user", "content": user_input})

            # ‚úÖ FIXED: Format prompt v·ªõi Llama 3 template ƒê√öNG CHU·∫®N
            prompt = format_llama3_prompt(system_prompt, history)

            # Log ƒë·ªô d√†i prompt
            logger.debug(f"üìù ƒê·ªô d√†i prompt: {len(prompt)} k√Ω t·ª±, {len(history)} turns")
            logger.debug(f"üìù Prompt preview (first 300 chars): {prompt[:300]}...")
            
            # Debug: Log to√†n b·ªô prompt n·∫øu c·∫ßn
            if logger.level <= logging.DEBUG:
                logger.debug("=" * 60)
                logger.debug("FULL PROMPT:")
                logger.debug("=" * 60)
                logger.debug(prompt)
                logger.debug("=" * 60)

            # ‚úÖ FIXED: Stop tokens cho Llama 3
            llama3_stop_tokens = [
                "<|eot_id|>",           # End of turn
                "<|end_of_text|>",      # End of text
                "<|start_header_id|>",  # Kh√¥ng ƒë·ªÉ model t·ª± t·∫°o header m·ªõi
            ]

            # Tham s·ªë sampling
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=Config.TOP_P,
                max_tokens=max_tokens,
                repetition_penalty=Config.REPETITION_PENALTY,
                stop=llama3_stop_tokens,  # ‚úÖ FIXED: D√πng stop tokens ƒë√∫ng cho Llama 3
            )

            logger.debug(f"‚öôÔ∏è Sampling params: temp={temperature}, max_tokens={max_tokens}, top_p={Config.TOP_P}")
            logger.debug(f"‚öôÔ∏è Stop tokens: {llama3_stop_tokens}")

            # Generate
            logger.debug("üîÑ ƒêang generate...")
            outputs = self.llm.generate([prompt], sampling_params)

            # DEBUG: Log chi ti·∫øt output
            logger.info(f"‚úÖ Generated output count: {len(outputs)}")
            
            if outputs and len(outputs) > 0:
                raw_text = outputs[0].outputs[0].text
                finish_reason = outputs[0].outputs[0].finish_reason
                
                logger.info(f"üìä Raw output length: {len(raw_text)} chars")
                logger.info(f"üìä Finish reason: {finish_reason}")
                logger.info(f"üìä Raw output preview (first 500 chars): '{raw_text[:500]}'")
                
                # ‚úÖ FIXED: L√†m s·∫°ch output
                # Lo·∫°i b·ªè special tokens n·∫øu c√≥
                cleaned_text = raw_text
                for token in llama3_stop_tokens:
                    cleaned_text = cleaned_text.replace(token, '')
                
                cleaned_text = cleaned_text.strip()
                
                # Ki·ªÉm tra n·∫øu output r·ªóng HO·∫∂C ch·ªâ ch·ª©a meta-instructions
                if not cleaned_text or len(cleaned_text) == 0:
                    logger.warning("‚ö†Ô∏è Model generate response R·ªñNG sau khi clean!")
                    return "Xin l·ªói, em ch∆∞a th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ph√π h·ª£p. Anh/ch·ªã th·ª≠ h·ªèi l·∫°i ƒë∆∞·ª£c kh√¥ng ·∫°?"
                
                # ‚úÖ FIXED: Ph√°t hi·ªán meta-instructions
                meta_keywords = [
                    "n·∫øu nh·∫≠n tag",
                    "ta s·∫Ω tr·∫£ l·ªùi",
                    "ta c√≥ th·ªÉ d·ª±a v√†o",
                    "ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi",
                    "quy t·∫Øc:",
                    "quy_t·∫Øc",
                    "nhi·ªám v·ª•:",
                    "nhi·ªám_v·ª•",
                    "b∆∞·ªõc 1:",
                    "ph∆∞∆°ng ph√°p",
                    "v√≠ d·ª• tr·∫£ l·ªùi",
                    "l∆∞u √Ω:",
                    "trong ƒëo·∫°n ƒë·ªëi tho·∫°i",
                    "tr√™n ƒë√¢y l√† v√≠ d·ª•",
                    "#1.", "#2.", "#3.", "#4.",  # Headers
                    "quan_tr·ªçng:",
                ]
                
                cleaned_lower = cleaned_text.lower()
                has_meta = any(kw in cleaned_lower for kw in meta_keywords)
                
                if has_meta:
                    logger.warning("‚ö†Ô∏è Model ƒëang generate META-INSTRUCTIONS thay v√¨ tr·∫£ l·ªùi!")
                    logger.warning(f"‚ö†Ô∏è Detected keywords in output: {[kw for kw in meta_keywords if kw in cleaned_lower]}")
                    logger.warning(f"‚ö†Ô∏è Output preview: {cleaned_text[:300]}")
                    
                    # Th·ª≠ extract c√¢u tr·∫£ l·ªùi th·ª±c s·ª± (n·∫øu c√≥)
                    # T√¨m d√≤ng ƒë·∫ßu ti√™n kh√¥ng ph·∫£i l√† instruction
                    lines = cleaned_text.split('\n')
                    for line in lines:
                        line_clean = line.strip()
                        line_lower = line_clean.lower()
                        
                        # B·ªè qua d√≤ng l√† instruction
                        if any(kw in line_lower for kw in meta_keywords):
                            continue
                        
                        # B·ªè qua d√≤ng r·ªóng ho·∫∑c ch·ªâ c√≥ d·∫•u
                        if not line_clean or line_clean in ['-', '*', '‚Ä¢']:
                            continue
                        
                        # N·∫øu t√¨m ƒë∆∞·ª£c d√≤ng h·ª£p l·ªá, tr·∫£ v·ªÅ
                        if len(line_clean) > 10:
                            logger.info(f"‚úÖ Extracted valid response from meta output: {line_clean[:100]}")
                            return line_clean
                    
                    # Kh√¥ng t√¨m ƒë∆∞·ª£c c√¢u tr·∫£ l·ªùi h·ª£p l·ªá
                    return "Ch√†o anh/ch·ªã! Em l√† tr·ª£ l√Ω c·ªßa S∆°n ƒê·ª©c D∆∞∆°ng. Em c√≥ th·ªÉ gi√∫p g√¨ cho anh/ch·ªã ·∫°?"
                
                logger.info(f"‚úÖ Generated successfully: {len(cleaned_text)} chars")
                logger.debug(f"‚úÖ Final output: {cleaned_text[:200]}...")
                
                return cleaned_text
                
            else:
                logger.error("‚ùå Model tr·∫£ v·ªÅ output array r·ªóng")
                return "Xin l·ªói, em g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu."

        except Exception as e:
            logger.error(f"‚ùå L·ªói khi generate: {e}", exc_info=True)
            logger.error(f"‚ùå User input was: {user_input}")
            logger.error(f"‚ùå System prompt length: {len(system_prompt)}")
            return "Xin l·ªói, em g·∫∑p l·ªói khi x·ª≠ l√Ω y√™u c·∫ßu c·ªßa anh/ch·ªã."

    def batch_generate(
            self,
            prompts: List[str],
            system_prompt: str,
            temperature: Optional[float] = None,
            max_tokens: Optional[int] = None,
    ) -> List[str]:
        """
        Batch generation cho nhi·ªÅu prompts c√πng l√∫c.

        L∆∞u √Ω: Kh√¥ng s·ª≠ d·ª•ng l·ªãch s·ª≠ h·ªôi tho·∫°i - t·ªët nh·∫•t cho c√°c queries ƒë∆°n gi·∫£n.

        Args:
            prompts: List c√°c chu·ªói user input
            system_prompt: H∆∞·ªõng d·∫´n system
            temperature: Temperature cho sampling
            max_tokens: S·ªë tokens t·ªëi ƒëa m·ªói generation

        Returns:
            List c√°c responses ƒë√£ generate (c√πng ƒë·ªô d√†i v·ªõi prompts)
        """

        # S·ª≠ d·ª•ng defaults n·∫øu kh√¥ng ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        if temperature is None:
            temperature = Config.DEFAULT_TEMPERATURE
        if max_tokens is None:
            max_tokens = Config.DEFAULT_MAX_TOKENS

        try:
            logger.info(f"üì¶ ƒêang batch generate {len(prompts)} prompts")

            # Format t·∫•t c·∫£ prompts v·ªõi Llama 3 format
            formatted = []
            for p in prompts:
                formatted.append(
                    format_llama3_prompt(  # ‚úÖ FIXED: D√πng format ƒë√∫ng
                        system_prompt,
                        [{"role": "user", "content": p}]
                    )
                )

            # Stop tokens cho Llama 3
            llama3_stop_tokens = [
                "<|eot_id|>",
                "<|end_of_text|>",
                "<|start_header_id|>",
            ]

            # Tham s·ªë sampling
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=Config.TOP_P,
                max_tokens=max_tokens,
                repetition_penalty=Config.REPETITION_PENALTY,
                stop=llama3_stop_tokens,  # ‚úÖ FIXED
            )

            # Generate batch
            outputs = self.llm.generate(formatted, sampling_params)

            # Tr√≠ch xu·∫•t v√† l√†m s·∫°ch t·∫•t c·∫£ texts
            results = []
            for o in outputs:
                text = o.outputs[0].text
                # Lo·∫°i b·ªè special tokens
                for token in llama3_stop_tokens:
                    text = text.replace(token, '')
                results.append(text.strip())

            logger.info(f"‚úÖ Batch generation ho√†n t·∫•t: {len(results)} responses")

            return results

        except Exception as e:
            logger.error(f"‚ùå L·ªói batch generation: {e}", exc_info=True)
            # Tr·∫£ v·ªÅ error messages cho t·∫•t c·∫£ prompts
            return ["L·ªói x·ª≠ l√Ω"] * len(prompts)

    def get_model_info(self) -> Dict:
        """
        L·∫•y th√¥ng tin v·ªÅ model ƒëang ƒë∆∞·ª£c load.
        
        Returns:
            Dict ch·ª©a th√¥ng tin model
        """
        return {
            "model_path": Config.MODEL_PATH,
            "dtype": Config.DTYPE,
            "max_model_len": Config.MAX_MODEL_LEN,
            "gpu_memory_utilization": Config.GPU_MEMORY_UTILIZATION,
            "tensor_parallel_size": Config.TENSOR_PARALLEL_SIZE,
            "max_num_seqs": Config.MAX_NUM_SEQS,
        }